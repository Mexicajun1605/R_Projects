---
title: "Text Mining Alice in Wonderland"
author: "Samuel Johnson"
date: '2022-09-05'
output: html_document
---
## Alice in Wonderland text mining

This project is to demonstrate some of what I have learned from David Robinson and Julia Silge's book on Text mining with a tidy approach. 

## Data, libraries, and tidying up

To begin with this project, we first have to call all of the libraries and packages used. The first package, gutenbergr, will mainly be used for downloading Alice. Tidytext is one of the more important texts that will be used. It includes a variety of tools for our textual analysis. 
Inside of Tidytext, we have a helpful list of words called Stop words. These are common grammatical and functional words that do not heavily impact the semantic meaning and hypersaturate any text. 
Finally we are going to tidy Alice up by defining the line numbers availabe and then we ungroup the text. Unnest_tokens is a function included with Tidytext that will converta Alice into a 2 X n data frame. The anti_join will remove the Stop words from Alice.
```{r}

library(gutenbergr)
library(tidyverse)
library(tidytext)
library(readr)
library(wordcloud)
library(reshape2)
library(scales)
#Downloading Alice in Wonderland
alice <- gutenberg_download(c(11))

#Defining the stop words
data("stop_words")

#Tidying Alice into a 2xn data frame by extracting all tokens
tidy_alice <- alice %>% 
  mutate(line_number = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)
```

## Finding frequency

Here we are defining the frequency of each token in Alice and plotting those that show up more than 50 times. The count function takes each token/word and sorts them in descending order. Ggplot2, a tidyverse library takes this order and plots it in a bar column. 

```{r}
alice_frequency <- tidy_alice %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 50) %>% 
  ggplot(aes(y = word, x = n)) +
  geom_col(fill = "blue")

alice_frequency
```

## Sentiment Analysis
Sentiment Analysis is a way of measuring how a text feels quantitatively. The two datasets we will be using are the NRC dataset and the Bing dataset. The NRC attributes 1 of 8 emotions to each word in a dictionary, for example, joy. We can load this using Tidytext and filter it using dplyr. 
Next we inner_join the sentiment we are looking for to count the words that correspond to joy in Alice. 

Afterwards we can track how the positivity or negativity of the work changes using the Bing dataset. This dataset ranks each word in a dictionary on a scale of -5 to 5. 

```{r pressure, echo=FALSE}
#Defining the sentiment we want to analyze
nrcjoy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

#Searching for that sentiment
tidy_alice %>% 
  inner_join(nrcjoy) %>% 
  count(word, sort = TRUE)

#Calculating the net sentiment using the Bing Lexicon
#This quantifies the sentiment of the book using a score of
#-5 to 5
alice_sentiment <- tidy_alice %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(index = line_number, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)

alice_sentiment

ggplot(alice_sentiment, aes(index, sentiment)) + 
    geom_col(show.legend = FALSE, fill = "blue")

#Binging Alice, we can arrive at scores of how positive/negative 
#Alice in Wonderland is
bing_alice <- tidy_alice %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()

#Graphing out the bing sentiment scores of the most common words

bing_alice %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(y = "Sentiment Contribution", 
       x = NULL) +
  theme(axis.text = element_text(angle = 45))
```

## Wordclouds

Finally the wordclouds will represent the most common words in the text by size. The larger the word, the more frequently it appears throughout the text. For example, "Alice" is the most common word, and therefore the largest. 

The second plot maps out the most negative words in green and the positive in orange. This shows us that the Mock Turtle plays an outsized role in the negativity of the text, since he gives us the most common negative word. 
```{r}
#Alice in Wonderland Wordcloud
#Here we will be working with Word clouds to show frequency and sentiment
tidy_alice %>% 
  anti_join(stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100))

#Sentiment wordcloud
tidy_alice %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(max.words = 100)

```

