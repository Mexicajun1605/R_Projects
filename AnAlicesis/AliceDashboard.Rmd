---
title: "An AnAlicesis of Alice in Wonderland"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
---

```{r setup, include=FALSE}
library(flexdashboard)
#This project was done to illustrate what I have learned from 
#David Robinson and Julia Silge's book on text mining
#The text that I have chosen for this project is Alice in Wonderland

library(gutenbergr)
library(tidyverse)
library(tidytext)
library(readr)
library(wordcloud)
library(reshape2)
library(scales)
library(DT)

#Downloading Alice in Wonderland and Through the Looking Glass
#We will also be making a vector of the two works
alice <- gutenberg_download(11, mirror = "http://mirrors.xmission.com/gutenberg/")

#Defining the stop words
data("stop_words")

#Tidying Alice, Looking Glass, and Lewis Carroll
#into a 2xn data frame by extracting all tokens
tidy_alice <- alice %>% 
  mutate(line_number = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)

#Plotting out and filtering words that show up more than 50 times
alice_frequency <- tidy_alice %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 50) %>% 
  ggplot(aes(y = word, x = n)) +
  geom_col(fill = "blue")

alice_frequency

#Defining the sentiment we want to analyze
nrcjoy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

#Searching for that sentiment
tidy_alice %>% 
  inner_join(nrcjoy) %>% 
  count(word, sort = TRUE)

#Calculating the net sentiment using the Bing Lexicon
#This quantifies the sentiment of the book using a score of
#-5 to 5
alice_sentiment <- tidy_alice %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(index = line_number, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)

alice_sentiment

ggplot(alice_sentiment, aes(index, sentiment)) + 
    geom_col(show.legend = FALSE, fill = "blue")

#Binging Alice, we can arrive at scores of how positive/negative 
#Alice in Wonderland is
bing_alice <- tidy_alice %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()

#Graphing out the bing sentiment scores of the most common words

bing_alice %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(y = "Sentiment Contribution", 
       x = NULL) +
  theme(axis.text = element_text(angle = 45))


#Alice in Wonderland Wordcloud
#Here we will be working with Word clouds to show frequency and sentiment
tidy_alice %>% 
  anti_join(stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100))

#Sentiment wordcloud
tidy_alice %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(max.words = 100)

#Creating a datatable that we can use to show 
datatable(tidy_alice %>% 
            anti_join(stop_words) %>% 
            count(word, sort = TRUE),
          extensions = "Buttons",
          options = list(
            dom = "Bfrtip",
            buttons = c("copy", "print", "csv"))
)

```
# AnAlicesis


## Column {.tabset}

### Sentiment Flow

```{r}
ggplot(alice_sentiment, aes(index, sentiment)) + 
    geom_col(show.legend = FALSE, fill = "blue")
```

### Visualizing sentiment proportions

```{r}
tidy_alice %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(max.words = 100)
```


## Column {data-width=400}


### Sentiment Contributions by Word

```{r}
bing_alice %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(y = "Sentiment Contribution", 
       x = NULL) +
  theme(axis.text = element_text(angle = 45))
```


## Column {.tabset}

### The most Common Words
```{r}
alice_frequency <- tidy_alice %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 50) %>% 
  ggplot(aes(y = word, x = n)) +
  geom_col(fill = "blue")

alice_frequency
```


### Visualizing the most Common words
```{r}
tidy_alice %>% 
  anti_join(stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100))
```

# Data

## Column

### Data Table for Frequency
```{r}
datatable(tidy_alice %>% 
            anti_join(stop_words) %>% 
            count(word, sort = TRUE),
          extensions = "Buttons",
          options = list(
          dom = "Bfrtip",
          buttons = c("copy", "print", "csv"))
          )
```